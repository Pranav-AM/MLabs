{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 234508,
     "status": "ok",
     "timestamp": 1553795814128,
     "user": {
      "displayName": "hemanth m",
      "photoUrl": "",
      "userId": "17315288822946816326"
     },
     "user_tz": -330
    },
    "id": "89byZb2dSqo0",
    "outputId": "c269c55c-3bf9-42c3-be37-76e19a52c507"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VWK4Sx_TS_1Z"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11384,
     "status": "ok",
     "timestamp": 1553795942711,
     "user": {
      "displayName": "hemanth m",
      "photoUrl": "",
      "userId": "17315288822946816326"
     },
     "user_tz": -330
    },
    "id": "riiXP79qZEAO",
    "outputId": "388cb1ac-9a2c-4b00-c50e-251f2bb22377"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csvfile = open('persondata.csv', 'wb')\n",
    "writer = csv.writer(csvfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IRRysF998_6v"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "\n",
    "def area(p1, p2, p3): \n",
    "    x1=p1[0]\n",
    "    y1=p1[1]\n",
    "    x2=p2[0]\n",
    "    y2=p2[1]\n",
    "    x3=p3[0]\n",
    "    y3=p3[1]\n",
    "    return abs((x1 * (y2 - y3) + x2 * (y3 - y1) + x3 * (y1 - y2)) / 2.0) \n",
    "\n",
    "def check(p1, p2, p3, p4, p): \n",
    "    \n",
    "    \n",
    "    # Calculate area of rectangle ABCD  \n",
    "    A = (area(p1, p2, p3) + area(p1, p4, p3)) \n",
    "\n",
    "    # Calculate area of triangle PAB  \n",
    "    A1 = area(p, p1, p2) \n",
    "\n",
    "    # Calculate area of triangle PBC  \n",
    "    A2 = area(p, p2, p3) \n",
    "\n",
    "    # Calculate area of triangle PCD  \n",
    "    A3 = area(p, p3, p4) \n",
    "\n",
    "    # Calculate area of triangle PAD  \n",
    "    A4 = area(p, p1, p4); \n",
    "\n",
    "    # Check if sum of A1, A2, A3  \n",
    "    # and A4 is same as A  \n",
    "    return (A == A1 + A2 + A3 + A4) \n",
    "\n",
    "class DetectorAPI:\n",
    "    def __init__(self, path_to_ckpt):\n",
    "        self.path_to_ckpt = path_to_ckpt\n",
    "\n",
    "        self.detection_graph = tf.Graph()\n",
    "        with self.detection_graph.as_default():\n",
    "            od_graph_def = tf.GraphDef()\n",
    "            with tf.gfile.GFile(self.path_to_ckpt, 'rb') as fid:\n",
    "                serialized_graph = fid.read()\n",
    "                od_graph_def.ParseFromString(serialized_graph)\n",
    "                tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "        self.default_graph = self.detection_graph.as_default()\n",
    "        self.sess = tf.Session(graph=self.detection_graph)\n",
    "\n",
    "        # Definite input and output Tensors for detection_graph\n",
    "        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "        # Each box represents a part of the image where a particular object was detected.\n",
    "        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "        # Each score represent how level of confidence for each of the objects.\n",
    "        # Score is shown on the result image, together with the class label.\n",
    "        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')\n",
    "\n",
    "    def processFrame(self, image):\n",
    "        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]\n",
    "        image_np_expanded = np.expand_dims(image, axis=0)\n",
    "        # Actual detection.\n",
    "        start_time = time.time()\n",
    "        (boxes, scores, classes, num) = self.sess.run(\n",
    "            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],\n",
    "            feed_dict={self.image_tensor: image_np_expanded})\n",
    "        end_time = time.time()\n",
    "\n",
    "        print(\"Elapsed Time:\", end_time-start_time)\n",
    "\n",
    "        im_height, im_width,_ = image.shape\n",
    "        boxes_list = [None for i in range(boxes.shape[1])]\n",
    "        for i in range(boxes.shape[1]):\n",
    "            boxes_list[i] = (int(boxes[0,i,0] * im_height),\n",
    "                        int(boxes[0,i,1]*im_width),\n",
    "                        int(boxes[0,i,2] * im_height),\n",
    "                        int(boxes[0,i,3]*im_width))\n",
    "\n",
    "        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])\n",
    "\n",
    "    def close(self):\n",
    "        self.sess.close()\n",
    "        self.default_graph.close()\n",
    "\n",
    "model_path = './person_detection/faster_rcnn_inception_v2_coco_2018_01_28/frozen_inference_graph.pb'\n",
    "odapi = DetectorAPI(path_to_ckpt=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import scipy\n",
    "import argparse\n",
    "import matplotlib\n",
    "# from torch import np\n",
    "import pylab as plt\n",
    "from joblib import Parallel, delayed\n",
    "import util\n",
    "import torch\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "from config_reader import config_reader\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import pickle\n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument('--t7_file', required=True)\n",
    "#parser.add_argument('--pth_file', required=True)\n",
    "#args = parser.parse_args()\n",
    "torch.set_num_threads(torch.get_num_threads())\n",
    "weight_name = './model/pose_model.pth'\n",
    "blocks = {}\n",
    "# find connection in the specified sequence, center 29 is in the position 15\n",
    "limbSeq = [[2,3], [2,6], [3,4], [4,5], [6,7], [7,8], [2,9], [9,10], \\\n",
    "           [10,11], [2,12], [12,13], [13,14], [2,1], [1,15], [15,17], \\\n",
    "           [1,16], [16,18], [3,17], [6,18]]\n",
    "# the middle joints heatmap correpondence\n",
    "mapIdx = [[31,32], [39,40], [33,34], [35,36], [41,42], [43,44], [19,20], [21,22], \\\n",
    "          [23,24], [25,26], [27,28], [29,30], [47,48], [49,50], [53,54], [51,52], \\\n",
    "          [55,56], [37,38], [45,46]]\n",
    "colors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0], \\\n",
    "          [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255], \\\n",
    "          [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85]]        \n",
    "block0  = [{'conv1_1':[3,64,3,1,1]},{'conv1_2':[64,64,3,1,1]},{'pool1_stage1':[2,2,0]},{'conv2_1':[64,128,3,1,1]},{'conv2_2':[128,128,3,1,1]},{'pool2_stage1':[2,2,0]},{'conv3_1':[128,256,3,1,1]},{'conv3_2':[256,256,3,1,1]},{'conv3_3':[256,256,3,1,1]},{'conv3_4':[256,256,3,1,1]},{'pool3_stage1':[2,2,0]},{'conv4_1':[256,512,3,1,1]},{'conv4_2':[512,512,3,1,1]},{'conv4_3_CPM':[512,256,3,1,1]},{'conv4_4_CPM':[256,128,3,1,1]}]\n",
    "blocks['block1_1']  = [{'conv5_1_CPM_L1':[128,128,3,1,1]},{'conv5_2_CPM_L1':[128,128,3,1,1]},{'conv5_3_CPM_L1':[128,128,3,1,1]},{'conv5_4_CPM_L1':[128,512,1,1,0]},{'conv5_5_CPM_L1':[512,38,1,1,0]}]\n",
    "blocks['block1_2']  = [{'conv5_1_CPM_L2':[128,128,3,1,1]},{'conv5_2_CPM_L2':[128,128,3,1,1]},{'conv5_3_CPM_L2':[128,128,3,1,1]},{'conv5_4_CPM_L2':[128,512,1,1,0]},{'conv5_5_CPM_L2':[512,19,1,1,0]}]\n",
    "\n",
    "for i in range(2,7):\n",
    "    blocks['block%d_1'%i]  = [{'Mconv1_stage%d_L1'%i:[185,128,7,1,3]},{'Mconv2_stage%d_L1'%i:[128,128,7,1,3]},{'Mconv3_stage%d_L1'%i:[128,128,7,1,3]},{'Mconv4_stage%d_L1'%i:[128,128,7,1,3]},\n",
    "{'Mconv5_stage%d_L1'%i:[128,128,7,1,3]},{'Mconv6_stage%d_L1'%i:[128,128,1,1,0]},{'Mconv7_stage%d_L1'%i:[128,38,1,1,0]}]\n",
    "    blocks['block%d_2'%i]  = [{'Mconv1_stage%d_L2'%i:[185,128,7,1,3]},{'Mconv2_stage%d_L2'%i:[128,128,7,1,3]},{'Mconv3_stage%d_L2'%i:[128,128,7,1,3]},{'Mconv4_stage%d_L2'%i:[128,128,7,1,3]},\n",
    "{'Mconv5_stage%d_L2'%i:[128,128,7,1,3]},{'Mconv6_stage%d_L2'%i:[128,128,1,1,0]},{'Mconv7_stage%d_L2'%i:[128,19,1,1,0]}]\n",
    "\n",
    "def make_layers(cfg_dict):\n",
    "    layers = []\n",
    "    for i in range(len(cfg_dict)-1):\n",
    "        one_ = cfg_dict[i]\n",
    "        for k,v in one_.iteritems():      \n",
    "            if 'pool' in k:\n",
    "                layers += [nn.MaxPool2d(kernel_size=v[0], stride=v[1], padding=v[2] )]\n",
    "            else:\n",
    "                conv2d = nn.Conv2d(in_channels=v[0], out_channels=v[1], kernel_size=v[2], stride = v[3], padding=v[4])\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "    one_ = cfg_dict[-1].keys()\n",
    "    k = one_[0]\n",
    "    v = cfg_dict[-1][k]\n",
    "    conv2d = nn.Conv2d(in_channels=v[0], out_channels=v[1], kernel_size=v[2], stride = v[3], padding=v[4])\n",
    "    layers += [conv2d]\n",
    "    return nn.Sequential(*layers)\n",
    "    \n",
    "layers = []\n",
    "for i in range(len(block0)):\n",
    "    one_ = block0[i]\n",
    "    for k,v in one_.iteritems():      \n",
    "        if 'pool' in k:\n",
    "            layers += [nn.MaxPool2d(kernel_size=v[0], stride=v[1], padding=v[2] )]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels=v[0], out_channels=v[1], kernel_size=v[2], stride = v[3], padding=v[4])\n",
    "            layers += [conv2d, nn.ReLU(inplace=True)]  \n",
    "models = {}           \n",
    "models['block0']=nn.Sequential(*layers)        \n",
    "\n",
    "for k,v in blocks.iteritems():\n",
    "    models[k] = make_layers(v)\n",
    "                \n",
    "class pose_model(nn.Module):\n",
    "    def __init__(self,model_dict,transform_input=False):\n",
    "        super(pose_model, self).__init__()\n",
    "        self.model0   = model_dict['block0']\n",
    "        self.model1_1 = model_dict['block1_1']        \n",
    "        self.model2_1 = model_dict['block2_1']  \n",
    "        self.model3_1 = model_dict['block3_1']  \n",
    "        self.model4_1 = model_dict['block4_1']  \n",
    "        self.model5_1 = model_dict['block5_1']  \n",
    "        self.model6_1 = model_dict['block6_1']  \n",
    "        self.model1_2 = model_dict['block1_2']        \n",
    "        self.model2_2 = model_dict['block2_2']  \n",
    "        self.model3_2 = model_dict['block3_2']  \n",
    "        self.model4_2 = model_dict['block4_2']  \n",
    "        self.model5_2 = model_dict['block5_2']  \n",
    "        self.model6_2 = model_dict['block6_2']\n",
    "        \n",
    "    def forward(self, x):    \n",
    "        out1 = self.model0(x)\n",
    "        out1_1 = self.model1_1(out1)\n",
    "        out1_2 = self.model1_2(out1)\n",
    "        out2  = torch.cat([out1_1,out1_2,out1],1)\n",
    "        out2_1 = self.model2_1(out2)\n",
    "        out2_2 = self.model2_2(out2)\n",
    "        out3   = torch.cat([out2_1,out2_2,out1],1)\n",
    "        out3_1 = self.model3_1(out3)\n",
    "        out3_2 = self.model3_2(out3)\n",
    "        out4   = torch.cat([out3_1,out3_2,out1],1)\n",
    "        out4_1 = self.model4_1(out4)\n",
    "        out4_2 = self.model4_2(out4)\n",
    "        out5   = torch.cat([out4_1,out4_2,out1],1)  \n",
    "        out5_1 = self.model5_1(out5)\n",
    "        out5_2 = self.model5_2(out5)\n",
    "        out6   = torch.cat([out5_1,out5_2,out1],1)          \n",
    "        out6_1 = self.model6_1(out6)\n",
    "        out6_2 = self.model6_2(out6)\n",
    "        return out6_1,out6_2        \n",
    "\n",
    "\n",
    "model = pose_model(models)     \n",
    "model.load_state_dict(torch.load(weight_name))\n",
    "#model.cuda()\n",
    "model.float()\n",
    "model.eval()\n",
    "\n",
    "param_, model_ = config_reader()\n",
    "personmapper=[]\n",
    "pm1=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bharat/.conda/envs/mlabs/lib/python2.7/site-packages/ipykernel_launcher.py:9: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time is 0.09393\n"
     ]
    }
   ],
   "source": [
    "#torch.nn.functional.pad(img pad, mode='constant', value=model_['padValue'])\n",
    "#for file1 in sorted(os.listdir(\"./test/frames\")):\n",
    "#   try:\n",
    "tic = time.time()\n",
    "test_image = './test/frames/'+'1076.jpg'\n",
    "#     test_image = '../frames/576.jpg'\n",
    "#test_image = 'a.jpg'\n",
    "oriImg = cv2.imread(test_image) # B,G,R order\n",
    "imageToTest = Variable(T.transpose(T.transpose(T.unsqueeze(torch.from_numpy(oriImg).float(),0),2,3),1,2),volatile=True)\n",
    "# oriImg = cv2.resize(oriImg,(1024,768))\n",
    "imoverlay = oriImg\n",
    "multiplier = [x * model_['boxsize'] / oriImg.shape[0] for x in param_['scale_search']]\n",
    "\n",
    "heatmap_avg = torch.zeros((len(multiplier),19,oriImg.shape[0], oriImg.shape[1]))\n",
    "paf_avg = torch.zeros((len(multiplier),38,oriImg.shape[0], oriImg.shape[1]))\n",
    "#print heatmap_avg.size()\n",
    "\n",
    "toc =time.time()\n",
    "print ('time is %.5f'%(toc-tic)) \n",
    "tic = time.time()\n",
    "for m in range(len(multiplier)):\n",
    "    scale = multiplier[m]\n",
    "    h = int(oriImg.shape[0]*scale)\n",
    "    w = int(oriImg.shape[1]*scale)\n",
    "    pad_h = 0 if (h%model_['stride']==0) else model_['stride'] - (h % model_['stride']) \n",
    "    pad_w = 0 if (w%model_['stride']==0) else model_['stride'] - (w % model_['stride'])\n",
    "    new_h = h+pad_h\n",
    "    new_w = w+pad_w\n",
    "    imageToTest = cv2.resize(oriImg, (0,0), fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)\n",
    "    imageToTest_padded, pad = util.padRightDownCorner(imageToTest, model_['stride'], model_['padValue'])\n",
    "    imageToTest_padded = np.transpose(np.float32(imageToTest_padded[:,:,:,np.newaxis]), (3,2,0,1))/256 - 0.5  #check\n",
    "    feed = Variable(T.from_numpy(imageToTest_padded))     \n",
    "    output1,output2 = model(feed)\n",
    "#         print output1.size()\n",
    "#         print output2.size()\n",
    "    #heatmap = nn.UpsamplingBilinear2d((oriImg.shape[0], oriImg.shape[1])).cpu()(output2)\n",
    "    #paf = nn.UpsamplingBilinear2d((oriImg.shape[0], oriImg.shape[1])).cpu()(output1)       \n",
    "    #heatmap_avg[m] = heatmap[0].data\n",
    "    #paf_avg[m] = paf[0].data  \n",
    "\n",
    "\n",
    "toc =time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     print 'time is %.5f'%(toc-tic) \n",
    "tic = time.time()\n",
    "\n",
    "heatmap_avg = T.transpose(T.transpose(T.squeeze(T.mean(heatmap_avg, 0)),0,1),1,2)\n",
    "paf_avg     = T.transpose(T.transpose(T.squeeze(T.mean(paf_avg, 0)),0,1),1,2) \n",
    "heatmap_avg=heatmap_avg.cpu().numpy()\n",
    "paf_avg    = paf_avg.cpu().numpy()\n",
    "toc =time.time()\n",
    "#     print 'time is %.5f'%(toc-tic) \n",
    "tic = time.time()\n",
    "all_peaks = []\n",
    "peak_counter = 0 \n",
    "for part in range(18):\n",
    "    map_ori = heatmap_avg[:,:,part]\n",
    "    map = gaussian_filter(map_ori, sigma=3)\n",
    "    map_left = np.zeros(map.shape)\n",
    "    map_left[1:,:] = map[:-1,:]\n",
    "    map_right = np.zeros(map.shape)\n",
    "    map_right[:-1,:] = map[1:,:]\n",
    "    map_up = np.zeros(map.shape)\n",
    "    map_up[:,1:] = map[:,:-1]\n",
    "    map_down = np.zeros(map.shape)\n",
    "    map_down[:,:-1] = map[:,1:]\n",
    "    peaks_binary = np.logical_and.reduce((map>=map_left, map>=map_right, map>=map_up, map>=map_down, map > param_['thre1']))\n",
    "#    peaks_binary = T.eq(\n",
    "#    peaks = zip(T.nonzero(peaks_binary)[0],T.nonzero(peaks_binary)[0])\n",
    "    peaks = zip(np.nonzero(peaks_binary)[1], np.nonzero(peaks_binary)[0]) # note reverse\n",
    "    peaks_with_score = [x + (map_ori[x[1],x[0]],) for x in peaks]\n",
    "    id = range(peak_counter, peak_counter + len(peaks))\n",
    "    peaks_with_score_and_id = [peaks_with_score[i] + (id[i],) for i in range(len(id))]\n",
    "    all_peaks.append(peaks_with_score_and_id)\n",
    "    peak_counter += len(peaks)\n",
    "# print(all_peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joints=[]\n",
    "for i in range(len(all_peaks)):\n",
    "    l=[]\n",
    "    for j in range(len(all_peaks[i])):\n",
    "        l.append([all_peaks[i][j][0],all_peaks[i][j][1]])\n",
    "    joints.append(l)\n",
    "# print(joints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_all = []\n",
    "special_k = []\n",
    "mid_num = 10\n",
    "for k in range(len(mapIdx)):\n",
    "    score_mid = paf_avg[:,:,[x-19 for x in mapIdx[k]]]\n",
    "    candA = all_peaks[limbSeq[k][0]-1]\n",
    "    candB = all_peaks[limbSeq[k][1]-1]\n",
    "    nA = len(candA)\n",
    "    nB = len(candB)\n",
    "    indexA, indexB = limbSeq[k]\n",
    "    if(nA != 0 and nB != 0):\n",
    "        connection_candidate = []\n",
    "        for i in range(nA):\n",
    "            for j in range(nB):\n",
    "                vec = np.subtract(candB[j][:2], candA[i][:2])\n",
    "                norm = math.sqrt(vec[0]*vec[0] + vec[1]*vec[1])\n",
    "                vec = np.divide(vec, norm)\n",
    "\n",
    "                startend = zip(np.linspace(candA[i][0], candB[j][0], num=mid_num), \\\n",
    "                               np.linspace(candA[i][1], candB[j][1], num=mid_num))\n",
    "                vec_x = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 0] \\\n",
    "                                  for I in range(len(startend))])\n",
    "                vec_y = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 1] \\\n",
    "                                  for I in range(len(startend))])\n",
    "                score_midpts = np.multiply(vec_x, vec[0]) + np.multiply(vec_y, vec[1])\n",
    "                score_with_dist_prior = sum(score_midpts)/len(score_midpts) + min(0.5*oriImg.shape[0]/norm-1, 0)\n",
    "                criterion1 = len(np.nonzero(score_midpts > param_['thre2'])[0]) > 0.8 * len(score_midpts)\n",
    "                criterion2 = score_with_dist_prior > 0\n",
    "                if criterion1 and criterion2:\n",
    "                    connection_candidate.append([i, j, score_with_dist_prior, score_with_dist_prior+candA[i][2]+candB[j][2]])\n",
    "        connection_candidate = sorted(connection_candidate, key=lambda x: x[2], reverse=True)\n",
    "        connection = np.zeros((0,5))\n",
    "        for c in range(len(connection_candidate)):\n",
    "            i,j,s = connection_candidate[c][0:3]\n",
    "            if(i not in connection[:,3] and j not in connection[:,4]):\n",
    "                connection = np.vstack([connection, [candA[i][3], candB[j][3], s, i, j]])\n",
    "                if(len(connection) >= min(nA, nB)):\n",
    "                    break\n",
    "        connection_all.append(connection)\n",
    "    else:\n",
    "        special_k.append(k)\n",
    "        connection_all.append([])\n",
    "# last number in each row is the total parts number of that person\n",
    "# the second last number in each row is the score of the overall configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = -1 * np.ones((0, 20))\n",
    "candidate = np.array([item for sublist in all_peaks for item in sublist])\n",
    "\n",
    "for k in range(len(mapIdx)):\n",
    "    if k not in special_k:\n",
    "        partAs = connection_all[k][:,0]\n",
    "        partBs = connection_all[k][:,1]\n",
    "        indexA, indexB = np.array(limbSeq[k]) - 1\n",
    "        for i in range(len(connection_all[k])): #= 1:size(temp,1)\n",
    "            found = 0\n",
    "            subset_idx = [-1, -1]\n",
    "            for j in range(len(subset)): #1:size(subset,1):\n",
    "                if subset[j][indexA] == partAs[i] or subset[j][indexB] == partBs[i]:\n",
    "                    subset_idx[found] = j\n",
    "                    found += 1\n",
    "            if found == 1:\n",
    "                j = subset_idx[0]\n",
    "                if(subset[j][indexB] != partBs[i]):\n",
    "                    subset[j][indexB] = partBs[i]\n",
    "                    subset[j][-1] += 1\n",
    "                    subset[j][-2] += candidate[partBs[i].astype(int), 2] + connection_all[k][i][2]\n",
    "            elif found == 2: # if found 2 and disjoint, merge them\n",
    "                j1, j2 = subset_idx\n",
    "                print \"found = 2\"\n",
    "                membership = ((subset[j1]>=0).astype(int) + (subset[j2]>=0).astype(int))[:-2]\n",
    "                if len(np.nonzero(membership == 2)[0]) == 0: #merge\n",
    "                    subset[j1][:-2] += (subset[j2][:-2] + 1)\n",
    "                    subset[j1][-2:] += subset[j2][-2:]\n",
    "                    subset[j1][-2] += connection_all[k][i][2]\n",
    "                    subset = np.delete(subset, j2, 0)\n",
    "                else: # as like found == 1\n",
    "                    subset[j1][indexB] = partBs[i]\n",
    "                    subset[j1][-1] += 1\n",
    "                    subset[j1][-2] += candidate[partBs[i].astype(int), 2] + connection_all[k][i][2]\n",
    "            # if find no partA in the subset, create a new subset\n",
    "            elif not found and k < 17:\n",
    "                row = -1 * np.ones(20)\n",
    "                row[indexA] = partAs[i]\n",
    "                row[indexB] = partBs[i]\n",
    "                row[-1] = 2\n",
    "                row[-2] = sum(candidate[connection_all[k][i,:2].astype(int), 2]) + connection_all[k][i][2]\n",
    "                subset = np.vstack([subset, row])\n",
    "# delete some rows of subset which has few parts occur\n",
    "deleteIdx = [];\n",
    "for i in range(len(subset)):\n",
    "    if subset[i][-1] < 4 or subset[i][-2]/subset[i][-1] < 0.4:\n",
    "        deleteIdx.append(i)\n",
    "subset = np.delete(subset, deleteIdx, axis=0)\n",
    "#canvas = cv2.imread(test_image) # B,G,R order\n",
    "canvas = imoverlay\n",
    "for i in range(18):\n",
    "    for j in range(len(all_peaks[i])):\n",
    "        cv2.circle(canvas, all_peaks[i][j][0:2], 4, colors[i], thickness=-1)\n",
    "stickwidth = 4\n",
    "posegraph={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2349
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32067,
     "status": "ok",
     "timestamp": 1553796794928,
     "user": {
      "displayName": "hemanth m",
      "photoUrl": "",
      "userId": "17315288822946816326"
     },
     "user_tz": -330
    },
    "id": "XmzaQk3J8_63",
    "outputId": "6e44fef1-5656-4831-fd22-cf87e665e24a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bharat/.conda/envs/mlabs/lib/python2.7/site-packages/ipykernel_launcher.py:9: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time is 0.27298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bharat/.conda/envs/mlabs/lib/python2.7/site-packages/torch/nn/modules/upsampling.py:225: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/bharat/.conda/envs/mlabs/lib/python2.7/site-packages/torch/nn/modules/upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    }
   ],
   "source": [
    "for i in range(17):\n",
    "    for n in range(len(subset)):\n",
    "        index = subset[n][np.array(limbSeq[i])-1]\n",
    "        if -1 in index:\n",
    "            continue\n",
    "        cur_canvas = canvas.copy()\n",
    "        Y = candidate[index.astype(int), 0]\n",
    "        X = candidate[index.astype(int), 1]\n",
    "        mX = np.mean(X)\n",
    "        mY = np.mean(Y)\n",
    "        length = ((X[0] - X[1]) ** 2 + (Y[0] - Y[1]) ** 2) ** 0.5\n",
    "        angle = math.degrees(math.atan2(X[0] - X[1], Y[0] - Y[1]))\n",
    "#         polygon = cv2.ellipse2Poly((int(mY),int(mX)), (int(length/2), stickwidth), int(angle), 0, 360, 1)\n",
    "        polygon = cv2.line(cur_canvas,(int(Y[0]),int(X[0])),(int(Y[1]),int(X[1])),colors[i],5)\n",
    "#         cv2.fillConvexPoly(cur_canvas, polygon, colors[i])\n",
    "#         cv2.fillConvexPoly(cur_canvas, polygon)\n",
    "        canvas = cv2.addWeighted(canvas, 0.4, cur_canvas, 0.6, 0)\n",
    "        posegraph[(int(Y[1]),int(X[1]))]=[int(Y[0]),int(X[0])]\n",
    "#         print(X,Y)\n",
    "# print posegraph\n",
    "# print len(posegraph)\n",
    "threshold = 0.7\n",
    "#     folder='./person_detection/frames'\n",
    "\n",
    "\n",
    "#img = cv2.resize(img, (350, 600))\n",
    "image=cv2.imread(test_image)\n",
    "boxes, scores, classes, num = odapi.processFrame(image)\n",
    "\n",
    "# Visualization of the results of a detection.\n",
    "pb=[]\n",
    "pb1=[]\n",
    "newcolors=[(255,0,0),(0,255,0),(0,0,255)]\n",
    "for i in range(len(boxes)):\n",
    "    # Class 1 represents human\n",
    "    if classes[i] == 1 and scores[i] > threshold:\n",
    "        box = boxes[i]\n",
    "        cv2.rectangle(canvas,(box[1],box[0]),(box[3],box[2]),newcolors[i],2)\n",
    "#         print(box)\n",
    "        pb.append([[box[1],box[2]],[box[1],box[0]],[box[3],box[0]],[box[3],box[2]]])\n",
    "#                 pb1.append([box[1],box[2],box[1],box[0],box[3],box[0],box[3],box[2]])\n",
    "#         cv2.imshow(\"preview\", img)\n",
    "# cv2.imwrite(\"./person_detection_result/\"+file,img)\n",
    "#     i=i+1\n",
    "#Parallel(n_jobs=1)(delayed(handle_one)(i) for i in range(18))\n",
    "persons=[]\n",
    "notassigned=[]\n",
    "for i in range(len(pb)):\n",
    "    persons.append([])\n",
    "for i in range(len(joints)):\n",
    "    for j in range(len(joints[i])):\n",
    "        flag=0\n",
    "        for k in range(len(pb)):\n",
    "            if check(pb[k][0],pb[k][1],pb[k][2],pb[k][3],joints[i][j]):\n",
    "                persons[k].append([joints[i][j],i])\n",
    "                flag=1\n",
    "                break\n",
    "        if flag==0:\n",
    "            notassigned.append([joints[i][j],i])\n",
    "# print persons\n",
    "# print ''\n",
    "# print notassigned\n",
    "# print ''\n",
    "\n",
    "for i in range(len(persons)):\n",
    "    print len(persons[j])\n",
    "# print posegraph.keys()\n",
    "for i in range(len(notassigned)):\n",
    "#     \n",
    "#     print ''\n",
    "#     print notassigned[i][0][0],notassigned[i][0][1]\n",
    "#     continue\n",
    "    if (notassigned[i][0][0],notassigned[i][0][1]) in posegraph.keys():\n",
    "        flag=True\n",
    "        x=posegraph[(notassigned[i][0][0],notassigned[i][0][1])]\n",
    "#         print x,notassigned[i]\n",
    "        while flag:\n",
    "            for j in range(len(persons)):\n",
    "                f1=0\n",
    "                for k in range(len(persons[j])):\n",
    "                    if x[0] == persons[j][k][0][0] and x[1] == persons[j][k][0][1]:\n",
    "                        flag=False\n",
    "#                         print '                          a                           '\n",
    "                        f1=1\n",
    "                        persons[j].insert(notassigned[i][1],notassigned[i])\n",
    "                        break\n",
    "                if f1==1:\n",
    "                    break\n",
    "            if (x[0],x[1]) in posegraph.keys():\n",
    "                x=posegraph[(x[0],x[1])]\n",
    "            else:\n",
    "                break\n",
    "\n",
    "for i in range(len(persons)):\n",
    "    print len(persons[j])\n",
    "# print notassigned\n",
    "li=[]\n",
    "for i in range(len(pb)):\n",
    "    li.append([pb[i][0][0],i])\n",
    "\n",
    "li.sort()\n",
    "for i in range(len(li)):\n",
    "    personmapper.append([file1,'person'+str(i),pb[li[i][1]],persons[li[i][1]]])\n",
    "    xyz=[file1,'person'+str(i),pb[li[i][1]],persons[li[i][1]]]\n",
    "    print personmapper[-1]\n",
    "    pms=[]\n",
    "    pms.append(xyz[0])\n",
    "    pms.append(xyz[1])\n",
    "    for j in range(len(xyz[2])):\n",
    "        pms.append(xyz[2][j][0])\n",
    "        pms.append(xyz[2][j][1])\n",
    "    k=0\n",
    "    for j in range(len(xyz[3])):\n",
    "        if xyz[3][j][1]==k:\n",
    "            pms.append(xyz[3][j][0][0])\n",
    "            pms.append(xyz[3][j][0][1])\n",
    "            k=k+1\n",
    "        else:\n",
    "            while xyz[3][j][1]>k:\n",
    "                pms.append(0)\n",
    "                pms.append(0)\n",
    "                k=k+1\n",
    "            pms.append(xyz[3][j][0][0])\n",
    "            pms.append(xyz[3][j][0][1])\n",
    "            k=k+1\n",
    "            while k<18:\n",
    "                pms.append(0)\n",
    "                pms.append(0)\n",
    "                k=k+1\n",
    "            pm1.append(pms)\n",
    "#         writer.writerow(pms)\n",
    "toc =time.time()\n",
    "print ('time is %.5f'%(toc-tic)) \n",
    "# cv2.imshow('img',canvas)\n",
    "cv2.imwrite('./pd/'+file1,canvas)\n",
    "\n",
    "#   except:\n",
    "#     print('error')\n",
    "torch.cuda.empty_cache() \n",
    "csvfile.close()\n",
    "# with open('./test/persondata.pkl', 'wb') as f:\n",
    "#     pickle.dump(personmapper, f)    \n",
    "# pm1=[]\n",
    "# for i in range(len(personmapper)):\n",
    "#   pms=[]\n",
    "\n",
    "#   print pms\n",
    "#   pm1.append(pms)\n",
    "# for i in range(len(pm1)):\n",
    "#   print pm1[i]\n",
    "# with open('./persondata1.pkl', 'wb') as f:\n",
    "#     pickle.dump(pm1, f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1499,
     "status": "ok",
     "timestamp": 1553797394476,
     "user": {
      "displayName": "hemanth m",
      "photoUrl": "",
      "userId": "17315288822946816326"
     },
     "user_tz": -330
    },
    "id": "I28GuHHrnN7t",
    "outputId": "84b0f107-c45a-4331-a81a-13297072c47c"
   },
   "outputs": [],
   "source": [
    "ann = [[\"1076.jpg\",\"person0\",44,578,209,578,209,159,44,159,150,219,117,225,125,244,134,313,167,295,90,245,73,304,125,334,115,352,121,438,91,539,97,347,108,442,100,516,145,208,149,209,125,203,134,203],\n",
    "     [\"1076.jpg\",\"person1\",248,588,408,588,408,164,248,164,310,221,345,238,331,259,324,316,280,343,359,258,375,326,312,332,344,357,332,455,330,532,347,368,352,460,368,549,315,214,318,212,330,212,335,210],\n",
    "     [\"1077.jpg\",\"person0\",56,570,234,570,234,149,56,149,157,210,128,221,143,243,166,303,199,282,93,243,80,301,128,326,125,349,126,434,98,533,101,344,113,433,100,511,149,200,155,200,134,197,140,200],\n",
    "     [\"1077.jpg\",\"person1\",247,587,404,587,404,158,247,158,318,218,347,234,339,256,342,321,298,349,363,254,375,318,315,328,338,363,334,448,334,527,348,369,358,459,372,546,319,208,322,209,335,209,340,206],\n",
    "     [\"1078.jpg\",\"person0\",57,573,254,573,254,156,57,156,163,209,134,221,156,247,194,294,229,286,97,245,80,298,130,328,128,343,121,439,98,532,97,338,101,435,102,511,158,198,159,200,139,200,147,200],\n",
    "     [\"1078.jpg\",\"person1\",262,589,402,589,402,160,262,160,317,219,347,235,336,257,336,317,298,349,365,256,378,321,318,328,343,363,338,453,336,527,347,370,359,460,371,546,319,210,323,210,335,211,343,206],\n",
    "     [\"1079.jpg\",\"person0\",47,575,254,575,254,161,161,47,161,211,133,226,155,253,193,299,226,293,93,249,79,304,131,330,123,347,122,443,95,538,93,346,100,439,98,517,156,203,162,206,141,205,146,206],\n",
    "     [\"1079.jpg\",\"person1\",256,587,409,587,409,167,256,167,313,224,344,240,336,260,327,322,287,338,362,257,376,327,319,334,343,358,336,453,333,529,346,373,356,467,369,551,316,216,319,216,333,216,342,212],\n",
    "     [\"1080.jpg\",\"person0\",44,581,246,581,246,159,44,159,157,211,124,228,148,251,186,298,222,295,95,245,75,301,128,337,120,348,117,442,97,537,94,347,99,435,101,512,150,203,157,203,136,203,142,206],\n",
    "     [\"1080.jpg\",\"person1\",248,585,401,585,401,168,248,168,313,226,345,236,335,264,330,322,290,341,364,261,375,323,318,334,332,364,334,455,331,534,346,370,351,463,367,549,316,217,320,216,331,216,339,214],\n",
    "     [\"1081.jpg\",\"person0\",49,576,207,576,207,162,49,162,158,224,127,226,133,253,141,315,180,329,91,246,75,301,133,335,109,352,119,438,93,536,102,349,100,442,98,515,154,217,159,217,137,206,143,210],\n",
    "     [\"1081.jpg\",\"person1\",248,585,404,585,404,169,248,169,312,224,343,239,333,265,323,319,280,341,366,259,378,322,322,333,333,361,332,453,335,532,349,369,356,463,370,547,315,216,320,218,332,216,338,211],\n",
    "     [\"1082.jpg\",\"person0\",56,547,186,547,186,163,56,163,154,229,123,225,120,250,112,316,155,351,94,237,75,297,137,335,106,353,114,439,97,537,97,350,108,426,98,511,153,219,157,220,136,204,141,213],\n",
    "     [\"1082.jpg\",\"person1\",221,586,406,586,406,165,221,165,310,226,342,237,324,260,303,305,247,299,369,257,382,321,319,334,327,353,333,455,335,528,345,359,351,459,371,550,312,217,317,217,332,218,340,214],\n",
    "     [\"1083.jpg\",\"person0\",49,570,180,570,180,165,49,165,154,227,121,224,119,249,117,320,154,353,98,233,71,294,129,335,106,350,113,441,96,540,95,345,100,447,99,513,152,217,157,219,138,203,143,210],\n",
    "     [\"1083.jpg\",\"person1\",201,586,413,586,413,168,201,168,304,226,340,235,313,259,281,280,231,277,367,255,387,320,322,333,333,360,334,451,333,529,353,368,356,458,371,547,308,219,311,218,329,217,338,213],\n",
    "     [\"1084.jpg\",\"person0\",48,575,210,575,210,160,48,160,161,226,127,224,131,252,155,315,184,329,94,235,73,292,126,332,114,344,119,434,98,528,99,348,101,434,103,506,160,216,165,220,145,200,150,209],\n",
    "     [\"1084.jpg\",\"person1\",206,582,410,582,410,166,206,166,306,225,345,235,316,257,282,276,231,275,368,253,391,320,326,333,336,356,335,449,337,527,355,364,360,455,371,548,308,216,312,216,330,218,334,208],\n",
    "     [\"1085.jpg\",\"person0\",51,571,215,571,215,162,51,162,167,227,134,225,143,253,169,310,197,336,94,236,72,291,116,328,111,342,118,435,100,528,98,345,93,437,100,510,163,215,169,219,149,202,154,209],\n",
    "     [\"1085.jpg\",\"person1\",216,585,412,585,412,159,216,159,313,221,350,232,327,253,301,286,249,277,374,251,392,320,328,329,346,358,335,446,336,525,354,364,359,458,371,545,317,213,321,213,339,212,344,203],\n",
    "     [\"1086.jpg\",\"person0\",49,569,217,569,217,162,49,162,166,227,133,224,146,253,172,310,199,335,98,236,73,290,119,327,116,338,121,435,96,529,95,341,107,432,102,508,163,218,167,220,151,204,152,212],\n",
    "     [\"1086.jpg\",\"person1\",218,581,413,581,413,162,218,162,311,222,350,235,326,254,299,286,246,278,371,251,390,318,328,330,341,349,335,450,334,527,354,366,361,461,372,546,315,216,319,214,338,214,343,205],\n",
    "     [\"1087.jpg\",\"person0\",45,575,223,575,223,161,45,161,163,226,131,226,139,254,161,311,196,329,93,244,76,297,123,328,111,344,115,435,96,539,93,348,99,438,101,511,160,215,163,217,141,204,146,208],\n",
    "     [\"1087.jpg\",\"person1\",259,583,418,583,418,159,259,159,329,230,366,233,349,257,345,318,300,313,386,256,401,321,336,331,359,359,344,451,333,529,370,375,373,464,372,550,331,221,335,218,354,215,355,205],\n",
    "     [\"1088.jpg\",\"person0\",50,575,228,575,228,161,50,161,158,225,124,223,119,245,129,307,180,310,109,245,102,307,130,332,101,345,107,443,96,533,99,351,105,447,100,515,153,211,158,215,138,203,136,204],\n",
    "     [\"1088.jpg\",\"person1\",259,590,423,590,423,162,259,162,332,234,370,237,355,258,362,315,316,320,394,256,405,326,343,334,363,360,346,456,336,531,371,375,375,464,375,551,335,227,330,228,358,217,355,214],\n",
    "     [\"1089.jpg\",\"person0\",56,572,212,572,212,154,56,154,153,218,120,219,101,237,104,303,160,297,113,239,113,300,137,330,93,345,98,438,97,532,99,344,100,447,103,514,158,209,149,209,134,197,127,200],\n",
    "     [\"1089.jpg\",\"person1\",281,583,428,583,428,165,281,165,338,236,376,233,358,257,365,312,327,319,398,257,413,324,346,335,371,360,351,457,339,525,376,370,377,465,377,545,343,226,334,228,364,214,358,218],\n",
    "     [\"1090.jpg\",\"person0\",50,577,209,577,209,153,50,153,151,214,119,218,97,240,101,302,162,297,105,234,108,297,130,327,98,345,95,456,99,533,102,357,101,446,102,507,155,205,147,205,132,193,123,197],\n",
    "     [\"1090.jpg\",\"person1\",287,585,430,585,430,167,287,167,342,236,383,235,363,257,368,313,329,319,403,257,419,327,351,336,369,362,357,449,342,530,377,376,384,463,379,548,344,228,339,232,369,216,364,217]]\n",
    "# res = pickle.load(open(\"./test/persondata1.pkl\",\"rb\"))\n",
    "res=pm1\n",
    "# for i in range(len(res)):\n",
    "#   print res[i][0],ann[i][0]\n",
    "# print res\n",
    "\n",
    "l=[x for x in range(11)]\n",
    "h=[]\n",
    "for k in range(len(l)):\n",
    "  count=0\n",
    "  c=0\n",
    "  for i in range(len(res)):\n",
    "    if res[i][0]==ann[i][0] and res[i][1]==ann[i][1]:\n",
    "      for j in range(2,len(res[i])):\n",
    "        c=c+1\n",
    "        if abs(ann[i][j]-res[i][j])>l[k]:\n",
    "    #       print ann[i][j]-res[i][j]\n",
    "          count=count+1\n",
    "#   print count,c\n",
    "  h.append(100-(count*100/c))\n",
    "print l,h\n",
    "l1=[x for x in range(11)]\n",
    "h1=[]\n",
    "for k in range(len(l1)):\n",
    "  count=0\n",
    "  c=0\n",
    "  for i in range(len(res)):\n",
    "    if res[i][0]==ann[i][0] and res[i][1]==ann[i][1]:\n",
    "      for j in range(2,len(res[i])):\n",
    "        c=c+1\n",
    "        if res[i][j]!=0 and abs(ann[i][j]-res[i][j])>l1[k]:\n",
    "    #       print ann[i][j]-res[i][j]\n",
    "          count=count+1\n",
    "#   print count,c\n",
    "  h1.append(100-(count*100/c))\n",
    "print l1,h1\n",
    "y_true=[]\n",
    "y_pred=[]\n",
    "for i in range(len(res)):\n",
    "  y_true.append(ann[i][2:])\n",
    "  y_pred.append(res[i][2:])\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse=mean_squared_error(y_true, y_pred,multioutput='uniform_average')\n",
    "print mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1039,
     "status": "ok",
     "timestamp": 1553796162082,
     "user": {
      "displayName": "hemanth m",
      "photoUrl": "",
      "userId": "17315288822946816326"
     },
     "user_tz": -330
    },
    "id": "v8zgvKihxsYJ",
    "outputId": "f4ff88b2-31b3-48ed-d0dd-2f2f762e8422"
   },
   "outputs": [],
   "source": [
    "print pm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1184,
     "status": "ok",
     "timestamp": 1553802440590,
     "user": {
      "displayName": "hemanth m",
      "photoUrl": "",
      "userId": "17315288822946816326"
     },
     "user_tz": -330
    },
    "id": "ppv-8Xv0d92M",
    "outputId": "5c9f4966-8e14-4b51-b74a-9f560350b18d"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(l, h) \n",
    "plt.xlabel('pixel threshold considering occlusion')\n",
    "plt.ylabel('accuracy')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(l1, h1) \n",
    "plt.xlabel('pixel threshold not considering occlusion')\n",
    "plt.ylabel('accuracy')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NIXXjcQSjhlC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "demo.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
