{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"framedemo.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python2","display_name":"Python 2"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"vYVpHUavtQbF","colab_type":"code","outputId":"f2c4b186-6b8c-4f5b-d59b-2eba89b4ac6e","executionInfo":{"status":"ok","timestamp":1553190762662,"user_tz":-330,"elapsed":757,"user":{"displayName":"hemanth m","photoUrl":"","userId":"17315288822946816326"}},"colab":{"base_uri":"https://localhost:8080/"}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"_78FK5X_tQa0","colab_type":"code","outputId":"540da959-ef6f-412c-8332-06c034f0ab48","executionInfo":{"status":"error","timestamp":1553045387943,"user_tz":-330,"elapsed":2555,"user":{"displayName":"hemanth m","photoUrl":"","userId":"17315288822946816326"}},"colab":{"base_uri":"https://localhost:8080/","height":674}},"cell_type":"code","source":["\n","\n","\n","import numpy as np\n","import tensorflow as tf\n","import cv2\n","import time\n","import os\n","\n","\n","class DetectorAPI:\n","    def __init__(self, path_to_ckpt):\n","        self.path_to_ckpt = path_to_ckpt\n","\n","        self.detection_graph = tf.Graph()\n","        with self.detection_graph.as_default():\n","            od_graph_def = tf.GraphDef()\n","            with tf.gfile.GFile(self.path_to_ckpt, 'rb') as fid:\n","                serialized_graph = fid.read()\n","                od_graph_def.ParseFromString(serialized_graph)\n","                tf.import_graph_def(od_graph_def, name='')\n","\n","        self.default_graph = self.detection_graph.as_default()\n","        self.sess = tf.Session(graph=self.detection_graph)\n","\n","        # Definite input and output Tensors for detection_graph\n","        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')\n","        # Each box represents a part of the image where a particular object was detected.\n","        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')\n","        # Each score represent how level of confidence for each of the objects.\n","        # Score is shown on the result image, together with the class label.\n","        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')\n","        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')\n","        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')\n","\n","    def processFrame(self, image):\n","        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]\n","        image_np_expanded = np.expand_dims(image, axis=0)\n","        # Actual detection.\n","        start_time = time.time()\n","        (boxes, scores, classes, num) = self.sess.run(\n","            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],\n","            feed_dict={self.image_tensor: image_np_expanded})\n","        end_time = time.time()\n","\n","        print(\"Elapsed Time:\", end_time-start_time)\n","\n","        im_height, im_width,_ = image.shape\n","        boxes_list = [None for i in range(boxes.shape[1])]\n","        for i in range(boxes.shape[1]):\n","            boxes_list[i] = (int(boxes[0,i,0] * im_height),\n","                        int(boxes[0,i,1]*im_width),\n","                        int(boxes[0,i,2] * im_height),\n","                        int(boxes[0,i,3]*im_width))\n","\n","        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])\n","\n","    def close(self):\n","        self.sess.close()\n","        self.default_graph.close()\n","\n","model_path = './person_detection/faster_rcnn_inception_v2_coco_2018_01_28/frozen_inference_graph.pb'\n","odapi = DetectorAPI(path_to_ckpt=model_path)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NotFoundError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mNotFoundError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-2-4866600f15a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./person_detection/faster_rcnn_inception_v2_coco_2018_01_28/frozen_inference_graph.pb'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0modapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDetectorAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_ckpt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-4866600f15a8>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_to_ckpt)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mod_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_to_ckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0mserialized_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0mod_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mod_graph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    123\u001b[0m       \u001b[0mstring\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mregular\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \"\"\"\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preread_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.pyc\u001b[0m in \u001b[0;36m_preread_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         self._read_buf = pywrap_tensorflow.CreateBufferedInputStream(\n\u001b[0;32m---> 85\u001b[0;31m             compat.as_bytes(self.__name), 1024 * 512, status)\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_prewrite_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.pyc\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotFoundError\u001b[0m: ./person_detection/faster_rcnn_inception_v2_coco_2018_01_28/frozen_inference_graph.pb; No such file or directory"]}]},{"metadata":{"id":"vBGPHA4ptsV6","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"mTHMVGIItufB","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"123K8EMdtQa8","colab_type":"code","outputId":"6753ab2f-04da-4970-972e-1af3926809d4","executionInfo":{"status":"error","timestamp":1553045452012,"user_tz":-330,"elapsed":4523,"user":{"displayName":"hemanth m","photoUrl":"","userId":"17315288822946816326"}},"colab":{"base_uri":"https://localhost:8080/","height":368}},"cell_type":"code","source":["import os\n","import re\n","import sys\n","import cv2\n","import math\n","import time\n","import scipy\n","import argparse\n","import matplotlib\n","from torch import np\n","import pylab as plt\n","from joblib import Parallel, delayed\n","import util\n","import torch\n","import torch as T\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from collections import OrderedDict\n","from config_reader import config_reader\n","from scipy.ndimage.filters import gaussian_filter\n","#parser = argparse.ArgumentParser()\n","#parser.add_argument('--t7_file', required=True)\n","#parser.add_argument('--pth_file', required=True)\n","#args = parser.parse_args()\n","torch.set_num_threads(torch.get_num_threads())\n","weight_name = './model/pose_model.pth'\n","blocks = {}\n","# find connection in the specified sequence, center 29 is in the position 15\n","limbSeq = [[2,3], [2,6], [3,4], [4,5], [6,7], [7,8], [2,9], [9,10], \\\n","           [10,11], [2,12], [12,13], [13,14], [2,1], [1,15], [15,17], \\\n","           [1,16], [16,18], [3,17], [6,18]]\n","# the middle joints heatmap correpondence\n","mapIdx = [[31,32], [39,40], [33,34], [35,36], [41,42], [43,44], [19,20], [21,22], \\\n","          [23,24], [25,26], [27,28], [29,30], [47,48], [49,50], [53,54], [51,52], \\\n","          [55,56], [37,38], [45,46]]\n","colors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0], \\\n","          [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255], \\\n","          [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85]]        \n","block0  = [{'conv1_1':[3,64,3,1,1]},{'conv1_2':[64,64,3,1,1]},{'pool1_stage1':[2,2,0]},{'conv2_1':[64,128,3,1,1]},{'conv2_2':[128,128,3,1,1]},{'pool2_stage1':[2,2,0]},{'conv3_1':[128,256,3,1,1]},{'conv3_2':[256,256,3,1,1]},{'conv3_3':[256,256,3,1,1]},{'conv3_4':[256,256,3,1,1]},{'pool3_stage1':[2,2,0]},{'conv4_1':[256,512,3,1,1]},{'conv4_2':[512,512,3,1,1]},{'conv4_3_CPM':[512,256,3,1,1]},{'conv4_4_CPM':[256,128,3,1,1]}]\n","blocks['block1_1']  = [{'conv5_1_CPM_L1':[128,128,3,1,1]},{'conv5_2_CPM_L1':[128,128,3,1,1]},{'conv5_3_CPM_L1':[128,128,3,1,1]},{'conv5_4_CPM_L1':[128,512,1,1,0]},{'conv5_5_CPM_L1':[512,38,1,1,0]}]\n","blocks['block1_2']  = [{'conv5_1_CPM_L2':[128,128,3,1,1]},{'conv5_2_CPM_L2':[128,128,3,1,1]},{'conv5_3_CPM_L2':[128,128,3,1,1]},{'conv5_4_CPM_L2':[128,512,1,1,0]},{'conv5_5_CPM_L2':[512,19,1,1,0]}]\n","\n","for i in range(2,7):\n","    blocks['block%d_1'%i]  = [{'Mconv1_stage%d_L1'%i:[185,128,7,1,3]},{'Mconv2_stage%d_L1'%i:[128,128,7,1,3]},{'Mconv3_stage%d_L1'%i:[128,128,7,1,3]},{'Mconv4_stage%d_L1'%i:[128,128,7,1,3]},\n","{'Mconv5_stage%d_L1'%i:[128,128,7,1,3]},{'Mconv6_stage%d_L1'%i:[128,128,1,1,0]},{'Mconv7_stage%d_L1'%i:[128,38,1,1,0]}]\n","    blocks['block%d_2'%i]  = [{'Mconv1_stage%d_L2'%i:[185,128,7,1,3]},{'Mconv2_stage%d_L2'%i:[128,128,7,1,3]},{'Mconv3_stage%d_L2'%i:[128,128,7,1,3]},{'Mconv4_stage%d_L2'%i:[128,128,7,1,3]},\n","{'Mconv5_stage%d_L2'%i:[128,128,7,1,3]},{'Mconv6_stage%d_L2'%i:[128,128,1,1,0]},{'Mconv7_stage%d_L2'%i:[128,19,1,1,0]}]\n","\n","def make_layers(cfg_dict):\n","    layers = []\n","    for i in range(len(cfg_dict)-1):\n","        one_ = cfg_dict[i]\n","        for k,v in one_.iteritems():      \n","            if 'pool' in k:\n","                layers += [nn.MaxPool2d(kernel_size=v[0], stride=v[1], padding=v[2] )]\n","            else:\n","                conv2d = nn.Conv2d(in_channels=v[0], out_channels=v[1], kernel_size=v[2], stride = v[3], padding=v[4])\n","                layers += [conv2d, nn.ReLU(inplace=True)]\n","    one_ = cfg_dict[-1].keys()\n","    k = one_[0]\n","    v = cfg_dict[-1][k]\n","    conv2d = nn.Conv2d(in_channels=v[0], out_channels=v[1], kernel_size=v[2], stride = v[3], padding=v[4])\n","    layers += [conv2d]\n","    return nn.Sequential(*layers)\n","    \n","layers = []\n","for i in range(len(block0)):\n","    one_ = block0[i]\n","    for k,v in one_.iteritems():      \n","        if 'pool' in k:\n","            layers += [nn.MaxPool2d(kernel_size=v[0], stride=v[1], padding=v[2] )]\n","        else:\n","            conv2d = nn.Conv2d(in_channels=v[0], out_channels=v[1], kernel_size=v[2], stride = v[3], padding=v[4])\n","            layers += [conv2d, nn.ReLU(inplace=True)]  \n","       \n","models = {}           \n","models['block0']=nn.Sequential(*layers)        \n","\n","for k,v in blocks.iteritems():\n","    models[k] = make_layers(v)\n","                \n","class pose_model(nn.Module):\n","    def __init__(self,model_dict,transform_input=False):\n","        super(pose_model, self).__init__()\n","        self.model0   = model_dict['block0']\n","        self.model1_1 = model_dict['block1_1']        \n","        self.model2_1 = model_dict['block2_1']  \n","        self.model3_1 = model_dict['block3_1']  \n","        self.model4_1 = model_dict['block4_1']  \n","        self.model5_1 = model_dict['block5_1']  \n","        self.model6_1 = model_dict['block6_1']  \n","        self.model1_2 = model_dict['block1_2']        \n","        self.model2_2 = model_dict['block2_2']  \n","        self.model3_2 = model_dict['block3_2']  \n","        self.model4_2 = model_dict['block4_2']  \n","        self.model5_2 = model_dict['block5_2']  \n","        self.model6_2 = model_dict['block6_2']\n","        \n","    def forward(self, x):    \n","        out1 = self.model0(x)\n","        out1_1 = self.model1_1(out1)\n","        out1_2 = self.model1_2(out1)\n","        out2  = torch.cat([out1_1,out1_2,out1],1)\n","        out2_1 = self.model2_1(out2)\n","        out2_2 = self.model2_2(out2)\n","        out3   = torch.cat([out2_1,out2_2,out1],1)\n","        out3_1 = self.model3_1(out3)\n","        out3_2 = self.model3_2(out3)\n","        out4   = torch.cat([out3_1,out3_2,out1],1)\n","        out4_1 = self.model4_1(out4)\n","        out4_2 = self.model4_2(out4)\n","        out5   = torch.cat([out4_1,out4_2,out1],1)  \n","        out5_1 = self.model5_1(out5)\n","        out5_2 = self.model5_2(out5)\n","        out6   = torch.cat([out5_1,out5_2,out1],1)          \n","        out6_1 = self.model6_1(out6)\n","        out6_2 = self.model6_2(out6)\n","        return out6_1,out6_2        \n","\n","\n","model = pose_model(models)     \n","model.load_state_dict(torch.load(weight_name))\n","model.cuda()\n","model.float()\n","model.eval()\n","\n","param_, model_ = config_reader()\n","\n","#torch.nn.functional.pad(img pad, mode='constant', value=model_['padValue'])\n","tic = time.time()\n","test_image = '../frames/121.jpg'\n","#test_image = 'a.jpg'\n","oriImg = cv2.imread(test_image) # B,G,R order\n","imageToTest = Variable(T.transpose(T.transpose(T.unsqueeze(torch.from_numpy(oriImg).float(),0),2,3),1,2),volatile=True).cuda()\n","# oriImg = cv2.resize(oriImg,(1024,768))\n","imoverlay = oriImg\n","multiplier = [x * model_['boxsize'] / oriImg.shape[0] for x in param_['scale_search']]\n","\n","heatmap_avg = torch.zeros((len(multiplier),19,oriImg.shape[0], oriImg.shape[1])).cuda()\n","paf_avg = torch.zeros((len(multiplier),38,oriImg.shape[0], oriImg.shape[1])).cuda()\n","#print heatmap_avg.size()\n","\n","toc =time.time()\n","print 'time is %.5f'%(toc-tic) \n","tic = time.time()\n","for m in range(len(multiplier)):\n","    scale = multiplier[m]\n","    h = int(oriImg.shape[0]*scale)\n","    w = int(oriImg.shape[1]*scale)\n","    pad_h = 0 if (h%model_['stride']==0) else model_['stride'] - (h % model_['stride']) \n","    pad_w = 0 if (w%model_['stride']==0) else model_['stride'] - (w % model_['stride'])\n","    new_h = h+pad_h\n","    new_w = w+pad_w\n","    imageToTest = cv2.resize(oriImg, (0,0), fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)\n","    imageToTest_padded, pad = util.padRightDownCorner(imageToTest, model_['stride'], model_['padValue'])\n","    imageToTest_padded = np.transpose(np.float32(imageToTest_padded[:,:,:,np.newaxis]), (3,2,0,1))/256 - 0.5  #check\n","    feed = Variable(T.from_numpy(imageToTest_padded)).cuda()      \n","    output1,output2 = model(feed)\n","    print output1.size()\n","    print output2.size()\n","    heatmap = nn.UpsamplingBilinear2d((oriImg.shape[0], oriImg.shape[1])).cuda()(output2)\n","    paf = nn.UpsamplingBilinear2d((oriImg.shape[0], oriImg.shape[1])).cuda()(output1)       \n","    heatmap_avg[m] = heatmap[0].data\n","    paf_avg[m] = paf[0].data  \n","    \n","    \n","toc =time.time()\n","print 'time is %.5f'%(toc-tic) \n","tic = time.time()\n","    \n","heatmap_avg = T.transpose(T.transpose(T.squeeze(T.mean(heatmap_avg, 0)),0,1),1,2).cuda() \n","paf_avg     = T.transpose(T.transpose(T.squeeze(T.mean(paf_avg, 0)),0,1),1,2).cuda() \n","heatmap_avg=heatmap_avg.cpu().numpy()\n","paf_avg    = paf_avg.cpu().numpy()\n","toc =time.time()\n","print 'time is %.5f'%(toc-tic) \n","tic = time.time()\n","all_peaks = []\n","peak_counter = 0 \n","for part in range(18):\n","    map_ori = heatmap_avg[:,:,part]\n","    map = gaussian_filter(map_ori, sigma=3)\n","    map_left = np.zeros(map.shape)\n","    map_left[1:,:] = map[:-1,:]\n","    map_right = np.zeros(map.shape)\n","    map_right[:-1,:] = map[1:,:]\n","    map_up = np.zeros(map.shape)\n","    map_up[:,1:] = map[:,:-1]\n","    map_down = np.zeros(map.shape)\n","    map_down[:,:-1] = map[:,1:]\n","    peaks_binary = np.logical_and.reduce((map>=map_left, map>=map_right, map>=map_up, map>=map_down, map > param_['thre1']))\n","#    peaks_binary = T.eq(\n","#    peaks = zip(T.nonzero(peaks_binary)[0],T.nonzero(peaks_binary)[0])\n","    peaks = zip(np.nonzero(peaks_binary)[1], np.nonzero(peaks_binary)[0]) # note reverse\n","    peaks_with_score = [x + (map_ori[x[1],x[0]],) for x in peaks]\n","    id = range(peak_counter, peak_counter + len(peaks))\n","    peaks_with_score_and_id = [peaks_with_score[i] + (id[i],) for i in range(len(id))]\n","    all_peaks.append(peaks_with_score_and_id)\n","    peak_counter += len(peaks)\n","     \n","connection_all = []\n","special_k = []\n","mid_num = 10\n","for k in range(len(mapIdx)):\n","    score_mid = paf_avg[:,:,[x-19 for x in mapIdx[k]]]\n","    candA = all_peaks[limbSeq[k][0]-1]\n","    candB = all_peaks[limbSeq[k][1]-1]\n","    nA = len(candA)\n","    nB = len(candB)\n","    indexA, indexB = limbSeq[k]\n","    if(nA != 0 and nB != 0):\n","        connection_candidate = []\n","        for i in range(nA):\n","            for j in range(nB):\n","                vec = np.subtract(candB[j][:2], candA[i][:2])\n","                norm = math.sqrt(vec[0]*vec[0] + vec[1]*vec[1])\n","                vec = np.divide(vec, norm)\n","                \n","                startend = zip(np.linspace(candA[i][0], candB[j][0], num=mid_num), \\\n","                               np.linspace(candA[i][1], candB[j][1], num=mid_num))\n","                vec_x = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 0] \\\n","                                  for I in range(len(startend))])\n","                vec_y = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 1] \\\n","                                  for I in range(len(startend))])\n","                score_midpts = np.multiply(vec_x, vec[0]) + np.multiply(vec_y, vec[1])\n","                score_with_dist_prior = sum(score_midpts)/len(score_midpts) + min(0.5*oriImg.shape[0]/norm-1, 0)\n","                criterion1 = len(np.nonzero(score_midpts > param_['thre2'])[0]) > 0.8 * len(score_midpts)\n","                criterion2 = score_with_dist_prior > 0\n","                if criterion1 and criterion2:\n","                    connection_candidate.append([i, j, score_with_dist_prior, score_with_dist_prior+candA[i][2]+candB[j][2]])\n","        connection_candidate = sorted(connection_candidate, key=lambda x: x[2], reverse=True)\n","        connection = np.zeros((0,5))\n","        for c in range(len(connection_candidate)):\n","            i,j,s = connection_candidate[c][0:3]\n","            if(i not in connection[:,3] and j not in connection[:,4]):\n","                connection = np.vstack([connection, [candA[i][3], candB[j][3], s, i, j]])\n","                if(len(connection) >= min(nA, nB)):\n","                    break\n","        connection_all.append(connection)\n","    else:\n","        special_k.append(k)\n","        connection_all.append([])\n","# last number in each row is the total parts number of that person\n","# the second last number in each row is the score of the overall configuration\n","subset = -1 * np.ones((0, 20))\n","candidate = np.array([item for sublist in all_peaks for item in sublist])\n","\n","for k in range(len(mapIdx)):\n","    if k not in special_k:\n","        partAs = connection_all[k][:,0]\n","        partBs = connection_all[k][:,1]\n","        indexA, indexB = np.array(limbSeq[k]) - 1\n","        for i in range(len(connection_all[k])): #= 1:size(temp,1)\n","            found = 0\n","            subset_idx = [-1, -1]\n","            for j in range(len(subset)): #1:size(subset,1):\n","                if subset[j][indexA] == partAs[i] or subset[j][indexB] == partBs[i]:\n","                    subset_idx[found] = j\n","                    found += 1\n","            if found == 1:\n","                j = subset_idx[0]\n","                if(subset[j][indexB] != partBs[i]):\n","                    subset[j][indexB] = partBs[i]\n","                    subset[j][-1] += 1\n","                    subset[j][-2] += candidate[partBs[i].astype(int), 2] + connection_all[k][i][2]\n","            elif found == 2: # if found 2 and disjoint, merge them\n","                j1, j2 = subset_idx\n","                print \"found = 2\"\n","                membership = ((subset[j1]>=0).astype(int) + (subset[j2]>=0).astype(int))[:-2]\n","                if len(np.nonzero(membership == 2)[0]) == 0: #merge\n","                    subset[j1][:-2] += (subset[j2][:-2] + 1)\n","                    subset[j1][-2:] += subset[j2][-2:]\n","                    subset[j1][-2] += connection_all[k][i][2]\n","                    subset = np.delete(subset, j2, 0)\n","                else: # as like found == 1\n","                    subset[j1][indexB] = partBs[i]\n","                    subset[j1][-1] += 1\n","                    subset[j1][-2] += candidate[partBs[i].astype(int), 2] + connection_all[k][i][2]\n","            # if find no partA in the subset, create a new subset\n","            elif not found and k < 17:\n","                row = -1 * np.ones(20)\n","                row[indexA] = partAs[i]\n","                row[indexB] = partBs[i]\n","                row[-1] = 2\n","                row[-2] = sum(candidate[connection_all[k][i,:2].astype(int), 2]) + connection_all[k][i][2]\n","                subset = np.vstack([subset, row])\n","# delete some rows of subset which has few parts occur\n","deleteIdx = [];\n","for i in range(len(subset)):\n","    if subset[i][-1] < 4 or subset[i][-2]/subset[i][-1] < 0.4:\n","        deleteIdx.append(i)\n","subset = np.delete(subset, deleteIdx, axis=0)\n","#canvas = cv2.imread(test_image) # B,G,R order\n","canvas = imoverlay\n","for i in range(18):\n","    for j in range(len(all_peaks[i])):\n","        cv2.circle(canvas, all_peaks[i][j][0:2], 4, colors[i], thickness=-1)\n","stickwidth = 4\n","for i in range(17):\n","    for n in range(len(subset)):\n","        index = subset[n][np.array(limbSeq[i])-1]\n","        if -1 in index:\n","            continue\n","        cur_canvas = canvas.copy()\n","        Y = candidate[index.astype(int), 0]\n","        X = candidate[index.astype(int), 1]\n","        mX = np.mean(X)\n","        mY = np.mean(Y)\n","        length = ((X[0] - X[1]) ** 2 + (Y[0] - Y[1]) ** 2) ** 0.5\n","        angle = math.degrees(math.atan2(X[0] - X[1], Y[0] - Y[1]))\n","        polygon = cv2.ellipse2Poly((int(mY),int(mX)), (int(length/2), stickwidth), int(angle), 0, 360, 1)\n","        cv2.fillConvexPoly(cur_canvas, polygon, colors[i])\n","        canvas = cv2.addWeighted(canvas, 0.4, cur_canvas, 0.6, 0)\n","threshold = 0.7\n","folder='./person_detection/frames'\n","\n","\n","#img = cv2.resize(img, (350, 600))\n","\n","boxes, scores, classes, num = odapi.processFrame(canvas)\n","\n","# Visualization of the results of a detection.\n","\n","for i in range(len(boxes)):\n","    # Class 1 represents human\n","    if classes[i] == 1 and scores[i] > threshold:\n","        box = boxes[i]\n","        cv2.rectangle(canvas,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)\n","\n","#         cv2.imshow(\"preview\", img)\n","# cv2.imwrite(\"./person_detection_result/\"+file,img)\n","#     i=i+1\n","#Parallel(n_jobs=1)(delayed(handle_one)(i) for i in range(18))\n","\n","toc =time.time()\n","print 'time is %.5f'%(toc-tic)     \n","cv2.imwrite('result2.png',canvas)   \n","\n"],"execution_count":0,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-3-36f0442a5ba2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpylab\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjoblib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name np","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"metadata":{"id":"1uXl28ZRtQbB","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","import cv2\n","import time\n","import os\n","\n","\n","class DetectorAPI:\n","    def __init__(self, path_to_ckpt):\n","        self.path_to_ckpt = path_to_ckpt\n","\n","        self.detection_graph = tf.Graph()\n","        with self.detection_graph.as_default():\n","            od_graph_def = tf.GraphDef()\n","            with tf.gfile.GFile(self.path_to_ckpt, 'rb') as fid:\n","                serialized_graph = fid.read()\n","                od_graph_def.ParseFromString(serialized_graph)\n","                tf.import_graph_def(od_graph_def, name='')\n","\n","        self.default_graph = self.detection_graph.as_default()\n","        self.sess = tf.Session(graph=self.detection_graph)\n","\n","        # Definite input and output Tensors for detection_graph\n","        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')\n","        # Each box represents a part of the image where a particular object was detected.\n","        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')\n","        # Each score represent how level of confidence for each of the objects.\n","        # Score is shown on the result image, together with the class label.\n","        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')\n","        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')\n","        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')\n","\n","    def processFrame(self, image):\n","        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]\n","        image_np_expanded = np.expand_dims(image, axis=0)\n","        # Actual detection.\n","        start_time = time.time()\n","        (boxes, scores, classes, num) = self.sess.run(\n","            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],\n","            feed_dict={self.image_tensor: image_np_expanded})\n","        end_time = time.time()\n","\n","        print(\"Elapsed Time:\", end_time-start_time)\n","\n","        im_height, im_width,_ = image.shape\n","        boxes_list = [None for i in range(boxes.shape[1])]\n","        for i in range(boxes.shape[1]):\n","            boxes_list[i] = (int(boxes[0,i,0] * im_height),\n","                        int(boxes[0,i,1]*im_width),\n","                        int(boxes[0,i,2] * im_height),\n","                        int(boxes[0,i,3]*im_width))\n","\n","        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])\n","\n","    def close(self):\n","        self.sess.close()\n","        self.default_graph.close()\n","\n","model_path = '.person_detection/faster_rcnn_inception_v2_coco_2018_01_28/frozen_inference_graph.pb'\n","odapi = DetectorAPI(path_to_ckpt=model_path)\n","img = cv2.imread('./frames/'+file)\n","#img = cv2.resize(img, (350, 600))\n","\n","boxes, scores, classes, num = odapi.processFrame(img)\n","\n","# Visualization of the results of a detection.\n","\n","for i in range(len(boxes)):\n","    # Class 1 represents human\n","    if classes[i] == 1 and scores[i] > threshold:\n","        box = boxes[i]\n","        cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)\n","\n","#         cv2.imshow(\"preview\", img)\n","cv2.imwrite(\"./person_detection_result/\"+file,img)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"d6EnXoOstQbI","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}